{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import pca\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flatten\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取MNIST数据\n",
    "* 两组数据\n",
    "* 1. train 1-2000, test 2001-4000\n",
    "* 2. train 2001-4000, test 1-2000\n",
    "* 数据是按0-9的顺序排列的，每个数字有两百个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(flag=1,normalize=False):\n",
    "    \"\"\"\n",
    "    flag: 选择两组数据中的某一个\n",
    "    normalize: 数据是否要正则化 (在训练某些model的时候好像有bug，待研究)\n",
    "    \n",
    "    return: X_train, Y_train, X_test, Y_test\n",
    "    \"\"\"\n",
    "    labels = get_data('digits4000_txt/digits4000_digits_labels.txt')\n",
    "    digits = get_data('digits4000_txt/digits4000_digits_vec.txt')\n",
    "\n",
    "    if flag==1:\n",
    "        X_train = digits[:2000]\n",
    "        Y_train = labels[:2000]\n",
    "        X_test = digits[2000:]\n",
    "        Y_test = labels[2000:]\n",
    "    else:\n",
    "        X_train = digits[2000:]\n",
    "        Y_train = labels[2000:]\n",
    "        X_test = digits[:2000]\n",
    "        Y_test = labels[:2000]\n",
    "\n",
    "    if normalize:\n",
    "        X_train = X_train/255\n",
    "        Y_train = Y_train/255\n",
    "        X_test = X_test/255\n",
    "        Y_test = Y_test/255\n",
    "\n",
    "    return np.array(X_train),np.array(Y_train),np.array(X_test),np.array(Y_test)\n",
    "\n",
    "def get_data(filename):\n",
    "    return np.loadtxt(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_data(flag=1,normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各种Classifier的实现\n",
    "\n",
    "### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier(train_x,train_y):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    model = KNeighborsClassifier()\n",
    "    model.fit(train_x, train_y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_classifier(train_x, train_y):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression(penalty='l2')\n",
    "    model.fit(train_x, train_y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_classifier(train_x, train_y):\n",
    "    from sklearn import tree\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    model.fit(train_x, train_y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classifier(train_x, train_y):\n",
    "    from sklearn.svm import SVC\n",
    "    model = SVC(kernel='rbf', gamma=0.01, C=1) # 选择kernel之后，rbf的准确率比其他的高，参数还没研究过\n",
    "    model.fit(train_x, train_y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model\n",
    "* 模型参数直接在这个里面改\n",
    "* CNN不需要进行PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_classifier(train_x, train_y):\n",
    "    train_X = train_x.reshape(-1, 1, 28, 28)\n",
    "    train_Y = np_utils.to_categorical(train_y, num_classes=10)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Convolution2D(filters=32,\n",
    "                                        kernel_size=5,\n",
    "                                        strides=1,\n",
    "                                        padding='same',\n",
    "                                        batch_input_shape=(None, 1, 28, 28),\n",
    "                                        data_format='channels_first'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2, \n",
    "                                       strides=2, \n",
    "                                       padding='same', \n",
    "                                       data_format='channels_first'))\n",
    "    model.add(Convolution2D(64, 5, \n",
    "                                        strides=1, \n",
    "                                        padding='same', \n",
    "                                        data_format='channels_first'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(2, 2, 'same', data_format='channels_first'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = Adam(lr=1e-4)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_X, train_Y, epochs=20, batch_size=64)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN model\n",
    "* 和CNN model相似，结构不一样\n",
    "* DNN可以进行PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_classifier(train_x, train_y):\n",
    "    batch_size = 100\n",
    "    nb_classes = 10\n",
    "    nb_epoch = 20\n",
    "    \n",
    "    train_y = np_utils.to_categorical(train_y, num_classes=10)\n",
    "    \n",
    "    input_dim = train_x.shape[1]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(input_dim,)))\n",
    "    \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
    "    model.fit(train_x, train_y, batch_size=batch_size, epochs=nb_epoch)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用PCA降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_model(train_x,components=600):\n",
    "    model = pca.PCA(n_components=components).fit(train_x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training和testing的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers = ['knn_classifier','logistic_regression_classifier','decision_tree_classifier','svm_classifier','cnn_classifier','dnn_classifier']\n",
    "classifiers = ['knn_classifier','logistic_regression_classifier','decision_tree_classifier','svm_classifier','dnn_classifier'] # test\n",
    "x_train, y_train, x_test, y_test = load_data(flag=1,normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_process(classifiers, x_train, y_train, x_test, y_test):\n",
    "    result = {}\n",
    "    for classifier in classifiers:\n",
    "        try:\n",
    "            print(\"=======================\")\n",
    "            print('Classifier: {}'.format(classifier))\n",
    "            temp_model = eval(classifier)(x_train,y_train) \n",
    "    \n",
    "            if classifier == 'cnn_classifier': # CNN需要转换一下数据格式\n",
    "                x_test_reshape = x_test.reshape(-1, 1, 28, 28)\n",
    "                y_test_reshape = np_utils.to_categorical(y_test, num_classes=10)\n",
    "                loss, accuracy = temp_model.evaluate(x_test_reshape, y_test_reshape)\n",
    "            else:\n",
    "                if classifier == 'dnn_classifier':\n",
    "                    y_test_reshape = np_utils.to_categorical(y_test, num_classes=10)\n",
    "                    loss, accuracy = temp_model.evaluate(x_test, y_test_reshape)\n",
    "                else:\n",
    "                    y_train_predict = temp_model.predict(x_train)\n",
    "                    training_accuracy = metrics.accuracy_score(y_train,y_train_predict)\n",
    "                    print('training accuracy: {}'.format(\"%.2f%%\"%( 100*training_accuracy)))\n",
    "                    y_predict = temp_model.predict(x_test)\n",
    "                    accuracy = metrics.accuracy_score(y_test, y_predict)\n",
    "                \n",
    "            print('testing accuracy: {}'.format(\"%.2f%%\"%( 100*accuracy)))\n",
    "            result[classifier] = accuracy\n",
    "        except:\n",
    "            print('+++++++++++++++++++++++++')\n",
    "            print('Error with {}.'.format(classifier))\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "            print('+++++++++++++++++++++++++')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 没有normalization和PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================\n",
      "Classifier: knn_classifier\n",
      "training accuracy: 94.50%\n",
      "testing accuracy: 91.70%\n",
      "=======================\n",
      "Classifier: logistic_regression_classifier\n",
      "training accuracy: 100.00%\n",
      "testing accuracy: 83.45%\n",
      "=======================\n",
      "Classifier: decision_tree_classifier\n",
      "training accuracy: 100.00%\n",
      "testing accuracy: 73.50%\n",
      "=======================\n",
      "Classifier: svm_classifier\n",
      "training accuracy: 100.00%\n",
      "testing accuracy: 90.10%\n",
      "=======================\n",
      "Classifier: dnn_classifier\n",
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 14.2963 - acc: 0.1115\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 0s 197us/step - loss: 14.4902 - acc: 0.1010\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 0s 199us/step - loss: 14.4902 - acc: 0.1010\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 0s 195us/step - loss: 14.4982 - acc: 0.1005\n",
      "Epoch 5/20\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 13.9985 - acc: 0.1310\n",
      "Epoch 6/20\n",
      "2000/2000 [==============================] - 0s 138us/step - loss: 13.9383 - acc: 0.1345\n",
      "Epoch 7/20\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 13.7996 - acc: 0.1435\n",
      "Epoch 8/20\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 13.5989 - acc: 0.1555\n",
      "Epoch 9/20\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 13.5751 - acc: 0.1575\n",
      "Epoch 10/20\n",
      "2000/2000 [==============================] - 0s 139us/step - loss: 13.4504 - acc: 0.1655\n",
      "Epoch 11/20\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 13.4238 - acc: 0.1670\n",
      "Epoch 12/20\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 13.6092 - acc: 0.1545\n",
      "Epoch 13/20\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 12.6798 - acc: 0.2125\n",
      "Epoch 14/20\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 12.5884 - acc: 0.2175\n",
      "Epoch 15/20\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 12.5664 - acc: 0.2195\n",
      "Epoch 16/20\n",
      "2000/2000 [==============================] - 0s 211us/step - loss: 12.7975 - acc: 0.2055\n",
      "Epoch 17/20\n",
      "2000/2000 [==============================] - 0s 212us/step - loss: 12.6901 - acc: 0.2125\n",
      "Epoch 18/20\n",
      "2000/2000 [==============================] - 0s 167us/step - loss: 12.4639 - acc: 0.2260\n",
      "Epoch 19/20\n",
      "2000/2000 [==============================] - 0s 240us/step - loss: 12.2544 - acc: 0.2385\n",
      "Epoch 20/20\n",
      "2000/2000 [==============================] - 0s 200us/step - loss: 12.0919 - acc: 0.2485\n",
      "2000/2000 [==============================] - 0s 92us/step\n",
      "testing accuracy: 25.70%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'knn_classifier': 0.917,\n",
       " 'logistic_regression_classifier': 0.8345,\n",
       " 'decision_tree_classifier': 0.735,\n",
       " 'svm_classifier': 0.901,\n",
       " 'dnn_classifier': 0.257}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_process(classifiers, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 有normalization，没有PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================\n",
      "Classifier: knn_classifier\n",
      "training accuracy: 94.50%\n",
      "testing accuracy: 91.70%\n",
      "=======================\n",
      "Classifier: logistic_regression_classifier\n",
      "training accuracy: 98.60%\n",
      "testing accuracy: 88.50%\n",
      "=======================\n",
      "Classifier: decision_tree_classifier\n",
      "training accuracy: 100.00%\n",
      "testing accuracy: 72.25%\n",
      "=======================\n",
      "Classifier: svm_classifier\n",
      "training accuracy: 100.00%\n",
      "testing accuracy: 90.10%\n",
      "=======================\n",
      "Classifier: dnn_classifier\n",
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 1s 281us/step - loss: 1.0501 - acc: 0.6635\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 0s 162us/step - loss: 0.4182 - acc: 0.8690\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 0s 149us/step - loss: 0.2935 - acc: 0.9080\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 1s 263us/step - loss: 0.2058 - acc: 0.9335\n",
      "Epoch 5/20\n",
      "2000/2000 [==============================] - 0s 205us/step - loss: 0.1513 - acc: 0.9575\n",
      "Epoch 6/20\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.1072 - acc: 0.9670\n",
      "Epoch 7/20\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 0.1043 - acc: 0.9675\n",
      "Epoch 8/20\n",
      "2000/2000 [==============================] - 0s 187us/step - loss: 0.0449 - acc: 0.9885\n",
      "Epoch 9/20\n",
      "2000/2000 [==============================] - 0s 179us/step - loss: 0.0370 - acc: 0.9895\n",
      "Epoch 10/20\n",
      "2000/2000 [==============================] - 1s 251us/step - loss: 0.0249 - acc: 0.9930\n",
      "Epoch 11/20\n",
      "2000/2000 [==============================] - 0s 173us/step - loss: 0.0294 - acc: 0.9885\n",
      "Epoch 12/20\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 0.0192 - acc: 0.9955\n",
      "Epoch 13/20\n",
      "2000/2000 [==============================] - 1s 260us/step - loss: 0.0350 - acc: 0.9890\n",
      "Epoch 14/20\n",
      "2000/2000 [==============================] - 0s 187us/step - loss: 0.0078 - acc: 0.9995\n",
      "Epoch 15/20\n",
      "2000/2000 [==============================] - 0s 140us/step - loss: 0.0313 - acc: 0.9910\n",
      "Epoch 16/20\n",
      "2000/2000 [==============================] - 0s 235us/step - loss: 0.0057 - acc: 0.9995\n",
      "Epoch 17/20\n",
      "2000/2000 [==============================] - 0s 225us/step - loss: 0.0077 - acc: 0.9975\n",
      "Epoch 18/20\n",
      "2000/2000 [==============================] - 0s 171us/step - loss: 0.0082 - acc: 0.9975\n",
      "Epoch 19/20\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 0.0120 - acc: 0.9955\n",
      "Epoch 20/20\n",
      "2000/2000 [==============================] - 0s 169us/step - loss: 0.0065 - acc: 0.9985\n",
      "2000/2000 [==============================] - 0s 112us/step\n",
      "testing accuracy: 93.70%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'knn_classifier': 0.917,\n",
       " 'logistic_regression_classifier': 0.885,\n",
       " 'decision_tree_classifier': 0.7225,\n",
       " 'svm_classifier': 0.901,\n",
       " 'dnn_classifier': 0.937}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_process(classifiers, x_train/255, y_train, x_test/255, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 没有normalization，有PCA\n",
    "* CNN不适用于PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练PCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = PCA_model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================\n",
      "Classifier: knn_classifier\n",
      "training accuracy: 94.50%\n",
      "testing accuracy: 91.70%\n",
      "=======================\n",
      "Classifier: logistic_regression_classifier\n",
      "training accuracy: 99.85%\n",
      "testing accuracy: 75.95%\n",
      "=======================\n",
      "Classifier: decision_tree_classifier\n",
      "training accuracy: 100.00%\n",
      "testing accuracy: 64.45%\n",
      "=======================\n",
      "Classifier: svm_classifier\n",
      "training accuracy: 100.00%\n",
      "testing accuracy: 90.10%\n",
      "=======================\n",
      "Classifier: dnn_classifier\n",
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 1s 285us/step - loss: 13.0490 - acc: 0.1900\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 0s 128us/step - loss: 11.0509 - acc: 0.3090\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 10.2640 - acc: 0.3560\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 9.4031 - acc: 0.4105\n",
      "Epoch 5/20\n",
      "2000/2000 [==============================] - 0s 126us/step - loss: 8.8793 - acc: 0.4425\n",
      "Epoch 6/20\n",
      "2000/2000 [==============================] - 0s 127us/step - loss: 8.6636 - acc: 0.4580\n",
      "Epoch 7/20\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 8.3827 - acc: 0.4775\n",
      "Epoch 8/20\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 8.4196 - acc: 0.4735\n",
      "Epoch 9/20\n",
      "2000/2000 [==============================] - 0s 136us/step - loss: 8.2628 - acc: 0.4840\n",
      "Epoch 10/20\n",
      "2000/2000 [==============================] - 0s 129us/step - loss: 8.0690 - acc: 0.4950\n",
      "Epoch 11/20\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 8.0612 - acc: 0.4970\n",
      "Epoch 12/20\n",
      "2000/2000 [==============================] - 0s 158us/step - loss: 7.9408 - acc: 0.5045\n",
      "Epoch 13/20\n",
      "2000/2000 [==============================] - 0s 173us/step - loss: 8.0656 - acc: 0.4975\n",
      "Epoch 14/20\n",
      "2000/2000 [==============================] - 1s 267us/step - loss: 7.7122 - acc: 0.5200\n",
      "Epoch 15/20\n",
      "2000/2000 [==============================] - 0s 146us/step - loss: 7.6962 - acc: 0.5205\n",
      "Epoch 16/20\n",
      "2000/2000 [==============================] - 0s 141us/step - loss: 7.6741 - acc: 0.5210\n",
      "Epoch 17/20\n",
      "2000/2000 [==============================] - 0s 133us/step - loss: 7.6702 - acc: 0.5230\n",
      "Epoch 18/20\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 7.6322 - acc: 0.5250\n",
      "Epoch 19/20\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 7.8189 - acc: 0.5130\n",
      "Epoch 20/20\n",
      "2000/2000 [==============================] - 0s 135us/step - loss: 7.4864 - acc: 0.5335\n",
      "2000/2000 [==============================] - 0s 108us/step\n",
      "testing accuracy: 51.40%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'knn_classifier': 0.917,\n",
       " 'logistic_regression_classifier': 0.7595,\n",
       " 'decision_tree_classifier': 0.6445,\n",
       " 'svm_classifier': 0.901,\n",
       " 'dnn_classifier': 0.514}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_pca = pca_model.transform(x_train)\n",
    "x_test_pca = pca_model.transform(x_test)\n",
    "temp_classifiers = ['knn_classifier','logistic_regression_classifier','decision_tree_classifier','svm_classifier','dnn_classifier'] # 没有CNN\n",
    "training_process(temp_classifiers, x_train_pca, y_train, x_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 有normalization和PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================\n",
      "Classifier: knn_classifier\n",
      "training accuracy: 94.55%\n",
      "testing accuracy: 91.70%\n",
      "=======================\n",
      "Classifier: logistic_regression_classifier\n",
      "training accuracy: 98.35%\n",
      "testing accuracy: 87.90%\n",
      "=======================\n",
      "Classifier: decision_tree_classifier\n",
      "training accuracy: 100.00%\n",
      "testing accuracy: 64.40%\n",
      "=======================\n",
      "Classifier: svm_classifier\n",
      "training accuracy: 100.00%\n",
      "testing accuracy: 90.10%\n",
      "=======================\n",
      "Classifier: dnn_classifier\n",
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 1s 277us/step - loss: 0.9646 - acc: 0.7305\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.3099 - acc: 0.9205\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 0s 105us/step - loss: 0.1685 - acc: 0.9590\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.1037 - acc: 0.9740\n",
      "Epoch 5/20\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.0497 - acc: 0.9905\n",
      "Epoch 6/20\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.0270 - acc: 0.9970\n",
      "Epoch 7/20\n",
      "2000/2000 [==============================] - 0s 106us/step - loss: 0.0180 - acc: 0.9975\n",
      "Epoch 8/20\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.0099 - acc: 1.0000\n",
      "Epoch 9/20\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.0071 - acc: 0.9995\n",
      "Epoch 10/20\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 11/20\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 12/20\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 13/20\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 14/20\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 15/20\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 16/20\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 5.9898e-04 - acc: 1.0000\n",
      "Epoch 17/20\n",
      "2000/2000 [==============================] - 0s 109us/step - loss: 8.6944e-04 - acc: 1.0000\n",
      "Epoch 18/20\n",
      "2000/2000 [==============================] - 0s 111us/step - loss: 5.3377e-04 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "2000/2000 [==============================] - 0s 110us/step - loss: 3.9045e-04 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "2000/2000 [==============================] - 0s 108us/step - loss: 2.7384e-04 - acc: 1.0000\n",
      "2000/2000 [==============================] - 0s 98us/step\n",
      "testing accuracy: 91.15%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'knn_classifier': 0.917,\n",
       " 'logistic_regression_classifier': 0.879,\n",
       " 'decision_tree_classifier': 0.644,\n",
       " 'svm_classifier': 0.901,\n",
       " 'dnn_classifier': 0.9115}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_model = PCA_model(x_train/255,components=400)\n",
    "x_train_normal_pca = pca_model.transform(x_train/255)\n",
    "x_test_normal_pca = pca_model.transform(x_test/255)\n",
    "temp_classifiers = ['knn_classifier','logistic_regression_classifier','decision_tree_classifier','svm_classifier','dnn_classifier'] # 没有CNN\n",
    "training_process(temp_classifiers, x_train_normal_pca, y_train, x_test_normal_pca, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
